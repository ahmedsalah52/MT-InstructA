{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import clip\n",
    "from tqdm import tqdm\n",
    "class CFG():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dataset_train_per = 0.8\n",
    "    lr   = 1e-4\n",
    "    weight_decay = 1e-3\n",
    "    patience = 1\n",
    "    factor = 0.8\n",
    "    epochs = 100\n",
    "    batch_size = 24\n",
    "    workers = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Metaworld_Dataset:\n",
    "    def __init__(self,json_file_dir,data_dir,images_transform,general_transform) -> None:\n",
    "        self.csv_file = json_file_dir\n",
    "        self.data_dir = data_dir\n",
    "        self.images_transform = images_transform\n",
    "        self.general_transform = general_transform\n",
    "        with open(json_file_dir, \"r\") as read_file:\n",
    "            data = json.load(read_file)\n",
    "    \n",
    "\n",
    "\n",
    "        self.data = data\n",
    "        self.num_seqs = len(self.data.keys())\n",
    "        self.seq_len = 200\n",
    "\n",
    "        self.instructons = {'button-press-topdown-v2':['press the button']}\n",
    "        self.max = 0\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.num_seqs * self.seq_len)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        #idx = str(idx) \n",
    "        seq_num = index//self.seq_len\n",
    "        idx     = (index - (seq_num*self.seq_len))\n",
    "\n",
    "        data     = self.data[str(seq_num)]['data'] \n",
    "        taskname = self.data[str(seq_num)]['task_name']\n",
    "        instruct = random.choice(self.instructons[taskname])\n",
    "\n",
    "        ret = {}\n",
    "       \n",
    "\n",
    "        step        = data['step'][idx]\n",
    "        prev_action = data['prev_action'][idx]\n",
    "        action      = data['action'][idx]\n",
    "        reward      = data['reward'][idx]\n",
    "        state       = data['state'][idx]\n",
    "    \n",
    "        images_dir = os.path.join(self.data_dir,'images',taskname,str(seq_num))\n",
    "        \n",
    "        images_dirs =  [images_dir+'/'+str(step)+'_corner.png'        ,       \n",
    "                        images_dir+'/'+str(step)+'_corner2.png'     , \n",
    "                        images_dir+'/'+str(step)+'_behindGripper.png',\n",
    "                        images_dir+'/'+str(step)+'_corner3.png'     , \n",
    "                        images_dir+'/'+str(step)+'_topview.png'     \n",
    "        ]\n",
    "        step_images = []\n",
    "        for i in range(len(images_dirs)):\n",
    "            image = Image.open(images_dirs[i])\n",
    "            image = self.images_transform(image)\n",
    "            step_images.append(image)\n",
    "\n",
    "        action = torch.tensor(action)\n",
    "        #action[0:3] = (action[0:3]+5)/10\n",
    "        action[action == -1] = 2\n",
    "\n",
    "\n",
    "        ret['image']       = torch.stack(step_images)\n",
    "        ret['action']      = action\n",
    "        ret['state']       = torch.tensor(state)\n",
    "        ret['prev_action'] = torch.tensor(prev_action)\n",
    "        ret['reward']      = torch.tensor(reward)\n",
    "        ret['caption']     = instruct\n",
    "        \n",
    "       \n",
    "        return ret\n",
    "    \n",
    "\n",
    "def prepare_batch(batch):\n",
    "    batch['image'] = batch['image'].permute(2,0,1,3,4,5)\n",
    "    \n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len):\n",
    "        super().__init__()\n",
    "        # Modified version from: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "        # max_len determines how far the position can have an effect on a token (window)\n",
    "        \n",
    "        # Info\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Encoding - From formula\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, 4, 5\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model) # 1000^(2i/dim_model)\n",
    "        \n",
    "        # PE(pos, 2i) = sin(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        \n",
    "        # PE(pos, 2i + 1) = cos(pos/1000^(2i/dim_model))\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Saving buffer (same as parameter without gradients needed)\n",
    "        pos_encoding = pos_encoding.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer(\"pos_encoding\",pos_encoding)\n",
    "        \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        # Residual connection + pos encoding\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Model from \"A detailed guide to Pytorch's nn.Transformer() module.\", by\n",
    "    Daniel Melchor: https://medium.com/@danielmelchor/a-detailed-guide-to-pytorchs-nn-transformer-module-c80afbc9ffb1\n",
    "    \"\"\"\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model,\n",
    "        num_heads,\n",
    "        num_encoder_layers,\n",
    "        dropout_p,\n",
    "        seq_length,\n",
    "        emp_length,\n",
    "        num_actions,\n",
    "        variations_per_action\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # INFO\n",
    "        self.model_type = \"Transformer\"\n",
    "        self.dim_model = dim_model\n",
    "\n",
    "        # LAYERS\n",
    "        self.positional_encoder = PositionalEncoding(\n",
    "            dim_model=dim_model, dropout_p=dropout_p, max_len=seq_length\n",
    "        )\n",
    "        #self.embedding = nn.Embedding(8, dim_model)\n",
    "       \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=dim_model, nhead=num_heads,dropout=0.1)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "        self.flatten = nn.Flatten(1)\n",
    "        self.outs    = [nn.Linear(seq_length*emp_length, variations_per_action).to(CFG.device) for i in range(num_actions)]\n",
    "        \n",
    "\n",
    "    def forward(self, src):\n",
    "        \n",
    "        src = self.positional_encoder(src)\n",
    "      \n",
    "        src = src.permute(1,0,2)\n",
    "        \n",
    "        transformer_out = self.transformer(src)\n",
    "        transformer_out = transformer_out.permute(1,0,2)\n",
    "        transformer_out = self.flatten(transformer_out)\n",
    "        \n",
    "        rets = []\n",
    "        for i in range(4):\n",
    "            out = self.outs[i](transformer_out)\n",
    "            rets.append(out)\n",
    "\n",
    "        #rets = torch.cat(rets,1)\n",
    "\n",
    "        return rets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self,language_text_model,policy_head,seq_length,emp_length):\n",
    "        super().__init__()\n",
    "\n",
    "        self.language_text_model = language_text_model\n",
    "        self.policy_head = policy_head\n",
    "        self.seq_length = seq_length\n",
    "        self.emp_length = emp_length\n",
    "        \n",
    "        self.language_text_model.eval()\n",
    "    def forward(self,batch):\n",
    "        batch_size,cams,ch,h,w  = batch['image'].shape\n",
    "        batch[\"image\"] = torch.flatten(batch[\"image\"], start_dim=0, end_dim=1)\n",
    "\n",
    "        image_features = self.language_text_model.encode_image(batch[\"image\"])\n",
    "        text_features = self.language_text_model.encode_text(batch[\"caption\"])\n",
    "        \n",
    "        image_features = torch.unflatten(image_features,dim = 0,sizes=(batch_size,cams))\n",
    "\n",
    "        embeddings = torch.cat([image_features,text_features[:,None,:]],dim=1)\n",
    "        embeddings = embeddings.flatten(1)\n",
    "        embeddings = embeddings.unflatten(-1,(self.seq_length,self.emp_length)) # batch 192 , 8\n",
    "\n",
    "        logits = self.policy_head(embeddings)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, lr_scheduler, step,criterion):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch['caption'] = clip.tokenize(batch['caption']).to(CFG.device)\n",
    "        batch['image']   = batch['image'].to(CFG.device)\n",
    "        batch['action'] = batch['action'].to(CFG.device)\n",
    "        logits = model(batch)\n",
    "       \n",
    "        loss = 0\n",
    "        for i in range(4):\n",
    "\n",
    "            loss += criterion(logits[i],batch['action'][:,i].to(torch.int64))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if step == \"batch\":\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        count = batch[\"image\"].size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "\n",
    "def valid_epoch(model, valid_loader,criterion):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for batch in tqdm_object:\n",
    "        batch['caption'] = clip.tokenize(batch['caption']).to(CFG.device)\n",
    "        batch['image']   = batch['image'].to(CFG.device)\n",
    "        batch['action'] = batch['action'].to(CFG.device)\n",
    "        logits = model(batch)\n",
    "       \n",
    "        loss = 0\n",
    "        for i in range(4):\n",
    "            loss += criterion(logits[i],batch['action'][:,i].to(torch.int64))\n",
    "        tqdm_object.set_postfix(valid_loss=loss.mean())\n",
    "    return loss_meter\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "def main():\n",
    "\n",
    "    policy_head = Transformer(\n",
    "    dim_model=8,\n",
    "    num_heads=2,\n",
    "    num_encoder_layers=3,\n",
    "    dropout_p=0.1,\n",
    "    seq_length = 384,\n",
    "    emp_length = 8,\n",
    "    num_actions= 4,\n",
    "    variations_per_action = 3\n",
    "    ).to(CFG.device) \n",
    "    clip_model , preprocess = clip.load(\"ViT-B/32\", device=CFG.device)\n",
    "    \n",
    "    policy = Policy(language_text_model=clip_model,\n",
    "                    policy_head=policy_head,\n",
    "                    seq_length=384,\n",
    "                    emp_length=8,\n",
    "                    )\n",
    "\n",
    "    actions_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "    dataset = Metaworld_Dataset('/media/ahmed/HDD/WorkSpace/master_thesis/repo/datasets/metaworld/single_env.json',\n",
    "                                '/media/ahmed/HDD/WorkSpace/master_thesis/repo/datasets/metaworld/',preprocess,actions_transforms)\n",
    "    dataset_length = len(dataset)\n",
    "    trainset_length = int(dataset_length * CFG.dataset_train_per)\n",
    "    print('dataset len:',dataset_length,' trainset len:',trainset_length)\n",
    "    train_set, val_valid = torch.utils.data.random_split(dataset, [trainset_length, dataset_length - trainset_length])\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=CFG.batch_size,shuffle=True, num_workers = CFG.workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(val_valid, batch_size=CFG.batch_size,shuffle=False,num_workers = CFG.workers)\n",
    "    \n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    params = [\n",
    "        {\"params\": policy.policy_head.parameters(), \"lr\": CFG.lr},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(params, weight_decay=CFG.weight_decay)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n",
    "    )\n",
    "    step = \"epoch\"\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        policy.policy_head.train()\n",
    "        train_loss = train_epoch(policy, train_loader, optimizer, lr_scheduler, step,criterion)\n",
    "        policy.eval()\n",
    "        with torch.no_grad():\n",
    "            valid_loss = valid_epoch(policy, valid_loader)\n",
    "            if valid_loss.avg < best_loss:\n",
    "                best_loss = valid_loss.avg\n",
    "                torch.save(policy.policy_head.state_dict(), \"best.pt\")\n",
    "                print(\"Saved Best Model!\")\n",
    "            \n",
    "        lr_scheduler.step(valid_loss.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset len: 200000  trainset len: 160000\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/6667 [01:09<5:20:12,  2.89s/it, lr=0.0001, train_loss=3.58]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main()\n",
      "Cell \u001b[0;32mIn[6], line 92\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m policy\u001b[39m.\u001b[39mpolicy_head\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 92\u001b[0m train_loss \u001b[39m=\u001b[39m train_epoch(policy, train_loader, optimizer, lr_scheduler, step,criterion)\n\u001b[1;32m     93\u001b[0m policy\u001b[39m.\u001b[39meval()\n\u001b[1;32m     94\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, optimizer, lr_scheduler, step, criterion)\u001b[0m\n\u001b[1;32m      2\u001b[0m loss_meter \u001b[39m=\u001b[39m AvgMeter()\n\u001b[1;32m      3\u001b[0m tqdm_object \u001b[39m=\u001b[39m tqdm(train_loader, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader))\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm_object:\n\u001b[1;32m      5\u001b[0m     batch[\u001b[39m'\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mtokenize(batch[\u001b[39m'\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(CFG\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m      6\u001b[0m     batch[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]   \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(CFG\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1329\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1294\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1293\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1294\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1295\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1296\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1133\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    108\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/multiprocessing/connection.py:931\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    928\u001b[0m     deadline \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mmonotonic() \u001b[39m+\u001b[39m timeout\n\u001b[1;32m    930\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     ready \u001b[39m=\u001b[39m selector\u001b[39m.\u001b[39;49mselect(timeout)\n\u001b[1;32m    932\u001b[0m     \u001b[39mif\u001b[39;00m ready:\n\u001b[1;32m    933\u001b[0m         \u001b[39mreturn\u001b[39;00m [key\u001b[39m.\u001b[39mfileobj \u001b[39mfor\u001b[39;00m (key, events) \u001b[39min\u001b[39;00m ready]\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[39m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_selector\u001b[39m.\u001b[39;49mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 5, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(batch[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     16\u001b[0m batch[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(batch[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m], start_dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, end_dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m text \u001b[39m=\u001b[39m clip\u001b[39m.\u001b[39mtokenize(batch[\u001b[39m'\u001b[39m\u001b[39mcaption\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     20\u001b[0m image_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode_image(batch[\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device))\n\u001b[1;32m     21\u001b[0m text_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mencode_text(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "actions_transforms = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=CFG.device)\n",
    "dataset = Metaworld_Dataset('/media/ahmed/HDD/WorkSpace/master_thesis/repo/datasets/metaworld/single_env.json','/media/ahmed/HDD/WorkSpace/master_thesis/repo/datasets/metaworld/',preprocess,actions_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=8,shuffle=True)\n",
    "model.eval()\n",
    "for batch in train_loader:\n",
    "    batch_size,cams,ch,h,w  = batch['image'].shape\n",
    "    print(batch['image'].shape)\n",
    "    batch[\"image\"] = torch.flatten(batch[\"image\"], start_dim=0, end_dim=1)\n",
    "\n",
    "    text = clip.tokenize(batch['caption']).to(device)\n",
    "\n",
    "    image_features = model.encode_image(batch[\"image\"].to(device))\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    image_features = torch.unflatten(image_features,dim = 0,sizes=(batch_size,cams))\n",
    "\n",
    "    print(image_features.shape,text_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
