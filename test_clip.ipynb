{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1.7923, device='cuda:0') tensor(-1.4802, device='cuda:0')\n",
      "torch.Size([1, 512]) torch.Size([3, 512])\n",
      "Label probs: [[0.7686  0.06018 0.1714 ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "\n",
    "image = preprocess(Image.fromarray(np.uint8(np.zeros((224, 224, 3))))).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"darkness\"]).to(device)\n",
    "print(image.min(),image.max())\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    print(image_features.shape, text_features.shape)\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|███████████████████████████████████▉   | 371M/402M [02:24<00:08, 3.82MiB/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"RN50x4\", device=device)\n",
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttentionTransformer(nn.Module):\n",
    "    def __init__(self, q_dim,cross_attention_dim,embed_dim, num_heads=1,dropout=0.2):\n",
    "        super(CrossAttentionTransformer, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim \n",
    "        total_embed_dim = num_heads * embed_dim\n",
    "        # Linear transformations for queries, keys, and values\n",
    "        self.query_linear = nn.Linear(q_dim, total_embed_dim)\n",
    "        self.key_linear = nn.Linear(cross_attention_dim, total_embed_dim)\n",
    "        self.value_linear = nn.Linear(cross_attention_dim, total_embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Output linear layer\n",
    "        self.out_linear = nn.Linear(total_embed_dim,cross_attention_dim)\n",
    "\n",
    "    def forward(self, x, conditional_x):\n",
    "        batch_size, len_x, _     = x.size()\n",
    "        _, len_conditional_x, _ = conditional_x.size()\n",
    "\n",
    "        # Linear transformations\n",
    "        query = self.query_linear(x)\n",
    "        key   = self.key_linear(conditional_x)\n",
    "        value = self.value_linear(conditional_x)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        query = query.view(batch_size, len_x            , self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key   =   key.view(batch_size, len_conditional_x, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, len_conditional_x, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        attended_values = torch.matmul(attention_weights, value)\n",
    "\n",
    "        # Reshape and concatenate heads\n",
    "        attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, len_x, -1)\n",
    "\n",
    "        # Apply output linear layer\n",
    "        output = self.out_linear(attended_values)\n",
    "        output = self.dropout(output)\n",
    "        output = output + x\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 320, 8])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "batch_size = 2\n",
    "sequence_length = 5*(512//8)\n",
    "emp = 8\n",
    "heads = 16\n",
    "embed_dim = 8 \n",
    "\n",
    "\n",
    "images_emps = torch.rand((2, sequence_length, emp))  # Batch size of 2, sequence length of 10, embedding dimension of 512\n",
    "text_emp    = torch.rand((2, 64, emp))               # Batch size of 2, sequence length of 8, embedding dimension of 512\n",
    "\n",
    "model = CrossAttentionTransformer(q_dim=emp, cross_attention_dim=emp,embed_dim=embed_dim, num_heads=heads)\n",
    "output = model(images_emps, text_emp)\n",
    "print(output.shape)  # Output shape: torch.Size([2, 10, 512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class CrossAttentionTransformerMultiLayer(nn.Module):\n",
    "    def __init__(self, num_layers,q_dim,cross_attention_dim,embed_dim, num_heads=1):\n",
    "        super(CrossAttentionTransformerMultiLayer, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            CrossAttentionTransformer(q_dim=q_dim, cross_attention_dim=cross_attention_dim,embed_dim=embed_dim, num_heads=num_heads) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, vectors1, vector2):\n",
    "        for layer in self.layers:\n",
    "            vectors1 = layer(vectors1, vector2)\n",
    "        return vectors1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 320, 8])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "batch_size = 2\n",
    "sequence_length = 5*(512//8)\n",
    "emp = 8\n",
    "heads = 16\n",
    "embed_dim = 8 \n",
    "layers = 2\n",
    "\n",
    "images_emps = torch.rand((2, sequence_length, emp))  # Batch size of 2, sequence length of 10, embedding dimension of 512\n",
    "text_emp    = torch.rand((2, 64, emp))               # Batch size of 2, sequence length of 8, embedding dimension of 512\n",
    "\n",
    "model = CrossAttentionTransformerMultiLayer(num_layers=layers,q_dim=emp, cross_attention_dim=emp,embed_dim=embed_dim, num_heads=heads)\n",
    "output = model(images_emps, text_emp)\n",
    "print(output.shape)  # Output shape: torch.Size([2, 10, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CrossAttentionTransformerMultiLayer(nn.Module):\n",
    "    def __init__(self, num_layers,q_dim,cross_attention_dim,embed_dim, num_heads=1,dropout=0.2):\n",
    "        super(CrossAttentionTransformerMultiLayer, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            CrossAttentionLayer(q_dim=q_dim, cross_attention_dim=cross_attention_dim,embed_dim=embed_dim, num_heads=num_heads,dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self,x, conditional_x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, conditional_x)\n",
    "        return x\n",
    "\n",
    "class CrossAttentionEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.att_head_emp = args.att_head_emp\n",
    "       \n",
    "        self.encoder = CrossAttentionTransformerMultiLayer(num_layers=args.neck_layers,\n",
    "                                                                q_dim=args.att_head_emp,\n",
    "                                                                cross_attention_dim=args.att_head_emp,\n",
    "                                                                embed_dim=args.att_head_emp,\n",
    "                                                                num_heads=args.n_heads,\n",
    "                                                                dropout=args.neck_dropout)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, input_x):\n",
    "        images_emps,text_emps,pos_emps = input_x\n",
    "        shape = images_emps.shape\n",
    "\n",
    "        images_emps = images_emps.reshape(shape[0],-1,self.att_head_emp)\n",
    "        text_emps   = text_emps.reshape(shape[0],-1,self.att_head_emp)\n",
    "        images_emps  = self.encoder(images_emps,text_emps)\n",
    "        images_emps = images_emps\n",
    "        text_emps   = text_emps\n",
    "\n",
    "        text_images_embeddings = torch.cat([images_emps,text_emps],dim=1)\n",
    "        text_images_embeddings = self.flatten(text_images_embeddings)\n",
    "        return torch.cat([text_images_embeddings,pos_emps],dim=1)\n",
    "         \n",
    "         \n",
    "    \n",
    "\n",
    "      \n",
    "    def get_opt_params(self):\n",
    "        return  [\n",
    "            {\"params\": self.encoder.parameters()}\n",
    "             ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.num_heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossAttentionEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, ff_hidden_size, dropout):\n",
    "        super(CrossAttentionEncoderLayer, self).__init__()\n",
    "        self.cross_attention = CrossAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, ff_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_size, embed_size),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, conditional_src, src_mask):\n",
    "        cross_attended_src = self.cross_attention(src, conditional_src, src, src_mask)\n",
    "\n",
    "        x = self.dropout(self.norm1(cross_attended_src + src))\n",
    "        forward = self.feed_forward(x)\n",
    "\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossAttentionEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        ff_hidden_size,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(CrossAttentionEncoder, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                CrossAttentionEncoderLayer(\n",
    "                    embed_size, num_heads, ff_hidden_size, dropout\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, conditional_src, mask):\n",
    "        N, seq_length = src.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(device)\n",
    "        src = self.dropout(self.word_embedding(src) + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, conditional_src, mask)\n",
    "\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(num_heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask=None):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values  = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
    "        keys    = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.num_heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossAttentionEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, dropout):\n",
    "        super(CrossAttentionEncoderLayer, self).__init__()\n",
    "        self.cross_attention = CrossAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, conditional_src , src_mask=None):\n",
    "        cross_attended_src = self.cross_attention(src, conditional_src, src, src_mask)\n",
    "\n",
    "        x = self.dropout(self.norm1(cross_attended_src + src))\n",
    "        forward = self.feed_forward(x)\n",
    "\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossAttentionEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(CrossAttentionEncoder, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                CrossAttentionEncoderLayer(\n",
    "                    embed_size, num_heads, dropout\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, conditional_src, mask=None):\n",
    "        N, seq_length,emps = src.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length)\n",
    "        src = self.dropout(src + self.position_embedding(positions))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, conditional_src, mask)\n",
    "\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 512])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src      = torch.zeros((2, 5, 512))\n",
    "cond_src = torch.zeros((2, 1, 512))\n",
    "\n",
    "\n",
    "model = CrossAttentionEncoder(embed_size=512,num_layers=2, num_heads=8, dropout=0.1,max_length=5)\n",
    "\n",
    "model(cond_src,src).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PerceiverModel' from 'transformers' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/ahmed/Metaworld/Metaworld/test_clip.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/Metaworld/Metaworld/test_clip.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ahmed/Metaworld/Metaworld/test_clip.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m PerceiverModel, PerceiverConfig\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/Metaworld/Metaworld/test_clip.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCrossAttentionPerceiver\u001b[39;00m(torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/Metaworld/Metaworld/test_clip.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, config):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PerceiverModel' from 'transformers' (unknown location)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import PerceiverModel, PerceiverConfig\n",
    "class CrossAttentionPerceiver(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CrossAttentionPerceiver, self).__init__()\n",
    "\n",
    "        self.perceiver = PerceiverModel(config)\n",
    "        \n",
    "    def forward(self, input1, input2, attention_mask1=None, attention_mask2=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input1: Input tensor for the first sequence.\n",
    "            input2: Input tensor for the second sequence.\n",
    "            attention_mask1: Attention mask for the first sequence (optional).\n",
    "            attention_mask2: Attention mask for the second sequence (optional).\n",
    "        Returns:\n",
    "            outputs: Model outputs.\n",
    "        \"\"\"\n",
    "        # Assuming both inputs have the same sequence length\n",
    "        input_shape = input1.size()\n",
    "\n",
    "        # Combine the two sequences\n",
    "        combined_input = torch.cat([input1, input2], dim=1)\n",
    "\n",
    "        # Combine the attention masks if provided\n",
    "        if attention_mask1 is not None and attention_mask2 is not None:\n",
    "            combined_attention_mask = torch.cat([attention_mask1, attention_mask2], dim=1)\n",
    "        else:\n",
    "            combined_attention_mask = None\n",
    "\n",
    "        # Forward pass through the Perceiver model\n",
    "        outputs = self.perceiver(\n",
    "            inputs_embeds=combined_input,\n",
    "            attention_mask=combined_attention_mask\n",
    "        )\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Example usage:\n",
    "config = PerceiverConfig()\n",
    "cross_attention_perceiver = CrossAttentionPerceiver(config)\n",
    "\n",
    "# Dummy input tensors\n",
    "input1 = torch.randn(1, 10, 768)  # Sequence length of 10, assuming embedding size is 768\n",
    "input2 = torch.randn(1, 10, 768)\n",
    "\n",
    "# Forward pass\n",
    "outputs = cross_attention_perceiver(input1, input2)\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "query should be unbatched 2D or batched 3D tensor but received 4-D query tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ahmed/Metaworld/Metaworld/test_clip.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/Metaworld/Metaworld/test_clip.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m input2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m512\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/Metaworld/Metaworld/test_clip.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#position_embedding = nn.Embedding(20, 512)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/Metaworld/Metaworld/test_clip.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#positions = torch.arange(0, 10).expand(1, 10)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ahmed/Metaworld/Metaworld/test_clip.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#input1 = input1 + position_embedding(positions)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ahmed/Metaworld/Metaworld/test_clip.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m model(query\u001b[39m=\u001b[39;49minput1,key\u001b[39m=\u001b[39;49minput2,value\u001b[39m=\u001b[39;49minput2)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.9/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1242\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1243\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1244\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1245\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1246\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1247\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1248\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1249\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1250\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1251\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.9/site-packages/torch/nn/functional.py:5227\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5196\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tens_ops):\n\u001b[1;32m   5197\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   5198\u001b[0m         multi_head_attention_forward,\n\u001b[1;32m   5199\u001b[0m         tens_ops,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5224\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   5225\u001b[0m     )\n\u001b[0;32m-> 5227\u001b[0m is_batched \u001b[39m=\u001b[39m _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)\n\u001b[1;32m   5229\u001b[0m \u001b[39m# For unbatched input, we unsqueeze at the expected batch-dim to pretend that the input\u001b[39;00m\n\u001b[1;32m   5230\u001b[0m \u001b[39m# is batched, run the computation and before returning squeeze the\u001b[39;00m\n\u001b[1;32m   5231\u001b[0m \u001b[39m# batch dimension so that the output doesn't carry this temporary batch dimension.\u001b[39;00m\n\u001b[1;32m   5232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batched:\n\u001b[1;32m   5233\u001b[0m     \u001b[39m# unsqueeze if the input is unbatched\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/mujoco/lib/python3.9/site-packages/torch/nn/functional.py:5054\u001b[0m, in \u001b[0;36m_mha_shape_check\u001b[0;34m(query, key, value, key_padding_mask, attn_mask, num_heads)\u001b[0m\n\u001b[1;32m   5051\u001b[0m             \u001b[39massert\u001b[39;00m attn_mask\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m expected_shape, \\\n\u001b[1;32m   5052\u001b[0m                 (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected `attn_mask` shape to be \u001b[39m\u001b[39m{\u001b[39;00mexpected_shape\u001b[39m}\u001b[39;00m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mattn_mask\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5053\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 5054\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m   5055\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mquery should be unbatched 2D or batched 3D tensor but received \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m.\u001b[39mdim()\u001b[39m}\u001b[39;00m\u001b[39m-D query tensor\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5057\u001b[0m \u001b[39mreturn\u001b[39;00m is_batched\n",
      "\u001b[0;31mAssertionError\u001b[0m: query should be unbatched 2D or batched 3D tensor but received 4-D query tensor"
     ]
    }
   ],
   "source": [
    "# Dummy input tensors\n",
    "from torch import nn\n",
    "import torch \n",
    "model = torch.nn.MultiheadAttention(512,8)\n",
    "input1 = torch.randn(1, 10,2, 512)  # Sequence length of 10, assuming embedding size is 768\n",
    "input2 = torch.randn(1, 10, 512)\n",
    "#position_embedding = nn.Embedding(20, 512)\n",
    "#positions = torch.arange(0, 10).expand(1, 10)\n",
    "#input1 = input1 + position_embedding(positions)\n",
    "\n",
    "model(query=input1,key=input2,value=input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super(CrossAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values  = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys    = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out  = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask=None):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "        \n",
    "        values = values.reshape(N, value_len, self.num_heads, self.head_dim)\n",
    "        keys     = keys.reshape(N, key_len, self.num_heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        values  = self.values(values)\n",
    "        keys    = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.num_heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossAttentionEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, dropout):\n",
    "        super(CrossAttentionEncoderLayer, self).__init__()\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_size, num_heads, dropout=dropout,batch_first=True) \n",
    "        #CrossAttention(embed_size, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_size, embed_size),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, conditional_src , src_mask=None):\n",
    "     \n",
    "\n",
    "        cross_attended_src,attn_output_weights = self.cross_attention(query=src, key=conditional_src, value=conditional_src)\n",
    "        \n",
    "       \n",
    "        x = self.dropout(self.norm1(cross_attended_src + src))\n",
    "        forward = self.feed_forward(x)\n",
    "\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "\n",
    "class CrossAttentionEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(CrossAttentionEncoder, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "        self.pos_encoder  = PositionalEncoding(embed_size, dropout=dropout, max_len=max_length)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                CrossAttentionEncoderLayer(embed_size, num_heads, dropout) for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, conditional_src, mask=None):\n",
    "        src = src.permute(1,0,2)\n",
    "        conditional_src = conditional_src.permute(1,0,2)\n",
    "\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, conditional_src, mask)\n",
    "        \n",
    "        \n",
    "        src = src.permute(1,0,2)\n",
    "\n",
    "        return src\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=2048, dropout = 0.1):\n",
    "        super().__init__() \n",
    "        # We set d_ff as a default to 2048\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.linear_1(x)))\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        self.norm_3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        self.dropout_3 = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attn_1 = nn.MultiheadAttention(heads, d_model)\n",
    "        self.attn_2 = nn.MultiheadAttention(heads, d_model)\n",
    "        self.ff = FeedForward(d_model).cuda()\n",
    "def forward(self, x, e_outputs, src_mask, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.attn_2(x2, e_outputs, e_outputs,\n",
    "        src_mask))\n",
    "        x2 = self.norm_3(x)\n",
    "        x = x + self.dropout_3(self.ff(x2))\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads), N)\n",
    "        self.norm = Norm(d_model)\n",
    "    def forward(self, trg, e_outputs, src_mask, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for i in range(self.N):\n",
    "            x = self.layers[i](x, e_outputs, src_mask, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads)\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "    def forward(self, src, trg, src_mask, trg_mask):\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "# we don't perform softmax on the output as this will be handled \n",
    "# automatically by our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norml mse loss: 2475.20068359375\n",
      "tensor([[[1.0000, 0.0080, 0.0100, 0.0000]]], grad_fn=<DivBackward0>)\n",
      "Relative MSE Loss: 0.3055410087108612\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RelativeMSELoss(nn.Module):\n",
    "    def __init__(self,mag_weight=0.005):\n",
    "        super(RelativeMSELoss, self).__init__()\n",
    "        self.mag_weight = mag_weight\n",
    "    def forward(self, predicted, ground_truth):\n",
    "        # Normalize sequences\n",
    "        norm_predicted    = F.normalize(predicted, dim=-1)\n",
    "        norm_ground_truth = F.normalize(ground_truth,dim=-1)\n",
    "\n",
    "        # Calculate MSE on normalized sequences\n",
    "        mse_loss = F.mse_loss(norm_predicted, norm_ground_truth)\n",
    "        \n",
    "        # calculate the magnitude of the predicted vector\n",
    "        mag_predicted    = torch.norm(predicted, dim=-1)\n",
    "        mag_ground_truth = torch.norm(ground_truth, dim=-1)\n",
    "        mag_loss = F.mse_loss(mag_predicted, mag_ground_truth)\n",
    "\n",
    "        print(norm_predicted)\n",
    "        return mse_loss + self.mag_weight * mag_loss\n",
    "\n",
    "# Example usage\n",
    "predictions = (torch.tensor([1.0, 0.25, 1.0, 0], requires_grad=True)).unsqueeze(0)\n",
    "ground_truth = torch.tensor([0.5, 0.25, 0.5, 0], requires_grad=False).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "print('norml mse loss:',F.mse_loss(predictions, ground_truth).item())\n",
    "loss_function = RelativeMSELoss(0.01)\n",
    "loss = loss_function(predictions.unsqueeze(0), ground_truth.unsqueeze(0))\n",
    "\n",
    "print(\"Relative MSE Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.4621, 0.7616, 0.4621]], grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000, -0.5000,  1.0000, -0.5000])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0.7, -0.35, 0.7, -0.35])\n",
    "x = x/torch.max(x)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "model, preprocess_image = clip.load('ViT-B/32',jit=False)\n",
    "image_encoder = model.encode_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "imgpath = 'generated_data/train/button-press-topdown-v2/0/1_0_4.jpg'\n",
    "image  = Image.open(imgpath).convert('RGB')\n",
    "image = preprocess_image(image).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'visualizations/clip_visual_transformer.png'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "target_model = model.visual\n",
    "\n",
    "y = target_model(image.type(model.dtype))\n",
    "# y = hidden_state @ target_model.proj , undo this projection\n",
    "out = make_dot(y.mean(), params=dict(model.named_parameters()))\n",
    "#save make_dot output as image\n",
    "out.render(\"visualizations/clip_visual_transformer\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "y = target_model(image.type(model.dtype))\n",
    "# y = hidden_state @ target_model.proj , undo this projection\n",
    "#y = y @ target_model.proj.T\n",
    "#plot y in a vector heatmap\n",
    "\n",
    "data = y.cpu().detach().numpy()\n",
    "x_axis = range(len(data[0]))\n",
    "\n",
    "\n",
    "fig, (ax,ax2)= plt.subplots( nrows=2,sharex=True)\n",
    "\n",
    "ax.imshow(data, cmap=\"plasma\", aspect=\"auto\")\n",
    "\n",
    "ax2.plot(x_axis, data[0], \"-b\", label=\"image\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plot the data vector in a ligand heatmap in ax2\n",
    "#ax2.legend(x_axis,data)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3200])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FiLMBlock(nn.Module):\n",
    "    def __init__(self,vision_dim , instruct_dim):\n",
    "        super(FiLMBlock, self).__init__()\n",
    "\n",
    "        # Linear layers for conditioning\n",
    "        self.conditioning_linear1 = nn.Linear(instruct_dim, vision_dim)\n",
    "        self.conditioning_linear2 = nn.Linear(vision_dim, vision_dim * 2)  # Two times for scale and shift\n",
    "        self.Relu = nn.ReLU()\n",
    "    def forward(self, vision_embeddings, instruction_vector):\n",
    "        # Apply conditioning\n",
    "        conditioning_output = self.conditioning_linear1(instruction_vector)\n",
    "        conditioning_output = self.Relu(conditioning_output)\n",
    "        conditioning_output = self.conditioning_linear2(conditioning_output)\n",
    "\n",
    "        # Split the conditioning output into scale and shift parameters\n",
    "\n",
    "        scale , shift = torch.split(conditioning_output, vision_embeddings.size(1), dim=1)\n",
    "      \n",
    "        # Apply FiLM modulation\n",
    "        vision_embeddings = vision_embeddings * scale + shift\n",
    "\n",
    "        return vision_embeddings\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, vision_dim , instruct_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # FiLM block\n",
    "        self.film_block = FiLMBlock(vision_dim , instruct_dim)\n",
    "\n",
    "        self.Linear1 = nn.Linear(vision_dim,vision_dim)\n",
    "                        \n",
    "        self.Linear2 = nn.Linear(vision_dim,vision_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(vision_dim)\n",
    "        self.Relu = nn.ReLU()\n",
    "     \n",
    "\n",
    "    def forward(self, vision_embeddings, instruction_vector):\n",
    "        \n",
    "        vision_embeddings = self.Linear1(vision_embeddings)\n",
    "        vision_embeddings = self.Relu(vision_embeddings)\n",
    "\n",
    "        x = self.Linear2(vision_embeddings)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.film_block(x,instruction_vector)\n",
    "        x = self.Relu(x)\n",
    "\n",
    "        # Add the residual connection\n",
    "        vision_embeddings += x\n",
    "        return vision_embeddings\n",
    "\n",
    "class Film(nn.Module):\n",
    "    def __init__(self,N_blocks,vision_dim , instruct_dim):\n",
    "        super(Film, self).__init__()\n",
    "\n",
    "        self.blocks = []\n",
    "        for i in range(N_blocks):\n",
    "            self.blocks.append(ResidualBlock(vision_dim , instruct_dim))\n",
    "\n",
    "        self.blocks = nn.ModuleList(self.blocks)\n",
    "        self.flatten = nn.Flatten()\n",
    "    def forward(self, input_x,cat=True):\n",
    "\n",
    "        vision_embeddings, instruction_vector, pos_emps = input_x\n",
    "        vision_embeddings = self.flatten(vision_embeddings)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            vision_embeddings = block(vision_embeddings, instruction_vector)\n",
    "\n",
    "        if not cat:\n",
    "            return vision_embeddings,instruction_vector,pos_emps\n",
    "        \n",
    "        return torch.cat([vision_embeddings,instruction_vector,pos_emps],dim=1)\n",
    "        \n",
    "    def get_opt_params(self):\n",
    "        return  [\n",
    "            {\"params\": self.parameters()}\n",
    "             ]\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "cams =5\n",
    "vision_dim = 512*cams  # Dimensionality of flattened vision embeddings\n",
    "instruct_dim = 512  # Dimensionality of the hidden layer in the FiLM block\n",
    "\n",
    "# Create Residual Block with FiLM\n",
    "film = Film(4,vision_dim, instruct_dim)\n",
    "\n",
    "# Dummy input tensors\n",
    "vision_embeddings = torch.randn(32,vision_dim)  # Assuming 2D vision embeddings\n",
    "instruction_vector = torch.randn(32, instruct_dim)\n",
    "pos_vector         = torch.randn(32, 128)\n",
    "\n",
    "# Forward pass through Residual Block\n",
    "output_embeddings = film((vision_embeddings, instruction_vector, pos_vector))\n",
    "print(output_embeddings.shape)  # Output shape: (32, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_size = 4\n",
    "seq_length = 5\n",
    "feats = 10\n",
    "returns_embeddings = torch.zeros((batch_size , seq_length , feats))\n",
    "state_embeddings   = torch.ones((batch_size , seq_length , feats))\n",
    "action_embeddings  = torch.zeros((batch_size , seq_length , feats)).fill_(2)\n",
    "stacked_inputs = torch.stack(\n",
    "            (returns_embeddings, state_embeddings, action_embeddings), dim=1\n",
    ").permute(0, 2, 1, 3).reshape(batch_size, 3*seq_length, feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRA(nn.Module):\n",
    "\n",
    "    def __init__(self, length=0, pool_size =1,n_layer=1,embed_dim=128,top_k=1, dropout_rate=0.0, init_prompts=\"zeros\",\n",
    "                 rank=4, mod_q=True, mod_v=True, mod_k=False, mod_ff=True, \n",
    "                 lora_alpha=None, log_mod_stats=False, **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.log_mod_stats = log_mod_stats\n",
    "        self.rank = rank\n",
    "        self.mod_v = mod_v\n",
    "        self.mod_q = mod_q\n",
    "        self.mod_k = mod_k\n",
    "        self.mod_ff = mod_ff\n",
    "        self.lora_alpha = lora_alpha if lora_alpha is not None else self.rank * 2\n",
    "        self._scaling = self.lora_alpha / self.rank\n",
    "        \n",
    "        length = 10\n",
    "        \n",
    "        self.length = length\n",
    "        self.pool_size = pool_size\n",
    "        self.n_layer = n_layer\n",
    "        self.embed_dim = embed_dim\n",
    "        self._setup_prompt()\n",
    "\n",
    "    @property\n",
    "    def scaling(self):\n",
    "        return self._scaling\n",
    "        \n",
    "    def _setup_prompt(self):\n",
    "        self.lora_a = nn.Parameter(torch.zeros((self.pool_size, self.n_layer, self.length, self.embed_dim, self.rank)))\n",
    "        self.lora_b = nn.Parameter(torch.zeros((self.pool_size, self.n_layer, self.length, self.rank, self.embed_dim)))\n",
    "        nn.init.kaiming_uniform_(self.lora_a, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_b)\n",
    "    \n",
    "    def extract_prompt(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx: torch.Tensor. Indices to lookup.\n",
    "\n",
    "        \"\"\"\n",
    "        # idx: [batch_size x 1]\n",
    "        # lora_a_batched: [n_layer x length x batch_size x rank x embed_dim]\n",
    "        # lora_b_batched: [n_layer x length x batch_size x embed_dim x rank]\n",
    "        lora_a_batched = self.lora_a[idx].permute(1,2,0,3,4)\n",
    "        lora_b_batched = self.lora_b[idx].permute(1,2,0,3,4)\n",
    "        lora_params = []\n",
    "        for a,b in zip(lora_a_batched,lora_b_batched):\n",
    "            qa,qb = a[0],b[0]\n",
    "            va,vb = a[1],b[1]\n",
    "            ffa1,ffb1 = a[2:3],b[2:6]\n",
    "            ffa2,ffb2 = a[6:10],b[6:7]\n",
    "\n",
    "            #permute mlp layers\n",
    "            ffa1 = ffa1.permute(1,2,0,3).flatten(start_dim=1,end_dim=2)\n",
    "            ffa2 = ffa2.permute(1,2,0,3).flatten(start_dim=1,end_dim=2)\n",
    "            ffb1 = ffb1.permute(1,2,0,3).flatten(start_dim=2,end_dim=3)\n",
    "            ffb2 = ffb2.permute(1,2,0,3).flatten(start_dim=2,end_dim=3)\n",
    "            \n",
    "\n",
    "            lora_params.append({\n",
    "                'q_ba' : torch.matmul(qa,qb),\n",
    "                'v_ba' : torch.matmul(va,vb),\n",
    "                'ff1_ba': torch.matmul(ffa1,ffb1),\n",
    "                'ff2_ba': torch.matmul(ffa2,ffb2)\n",
    "\n",
    "            })\n",
    "            \n",
    "        return lora_params\n",
    "    \n",
    "\n",
    "    def add_dropout(self, batched_prompt):\n",
    "        return batched_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_ba torch.Size([5, 128, 128])\n",
      "v_ba torch.Size([5, 128, 128])\n",
      "ff1_ba torch.Size([5, 128, 512])\n",
      "ff2_ba torch.Size([5, 512, 128])\n",
      "q_ba torch.Size([5, 128, 128])\n",
      "v_ba torch.Size([5, 128, 128])\n",
      "ff1_ba torch.Size([5, 128, 512])\n",
      "ff2_ba torch.Size([5, 512, 128])\n",
      "q_ba torch.Size([5, 128, 128])\n",
      "v_ba torch.Size([5, 128, 128])\n",
      "ff1_ba torch.Size([5, 128, 512])\n",
      "ff2_ba torch.Size([5, 512, 128])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "\n",
    "lora = LoRA(pool_size=10,n_layer=3)\n",
    "lora._setup_prompt()\n",
    "idx = torch.tensor([1,2,3,4,5])\n",
    "\n",
    "lora_params = lora.extract_prompt(idx)\n",
    "\n",
    "for layer in lora_params:\n",
    "    for k,v in layer.items():\n",
    "        print(k,v.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "import torch.nn.functional as F\n",
    "class keys_pool(nn.Module):\n",
    "    def __init__(self, n_keys,d_dim,pool_size):\n",
    "        super().__init__()\n",
    "        self.key_encoder = nn.Embedding(n_keys,d_dim)\n",
    "        # init the vector with mean 0 and std 1\n",
    "        self.query = nn.Parameter(torch.randn((pool_size,d_dim)))\n",
    "        #nn.init.normal_(self.query,mean=0,std=1)\n",
    "    def forward(self,idx):\n",
    "        encoded_keys = self.key_encoder(idx)\n",
    "        #calculate cosine similarity\n",
    "        #encoded_keys = encoded_keys / encoded_keys.norm(dim=-1, keepdim=True)\n",
    "        #self.query   = self.query / self.query.norm(dim=-1, keepdim=True)\n",
    "        #similarity = (100.0 * encoded_keys @ self.query.T).softmax(dim=-1)\n",
    "\n",
    "        prompt_norm  = self.l2_normalize(self.query  , dim=1)  # Pool_size, C\n",
    "        x_embed_norm = self.l2_normalize(encoded_keys, dim=1)  # B, C\n",
    "        similarity = torch.matmul(x_embed_norm, prompt_norm.t())  # B, Pool_size\n",
    "\n",
    "        return similarity\n",
    "    @staticmethod\n",
    "    def l2_normalize(x, dim=None, epsilon=1e-12):\n",
    "        return torch.nn.functional.normalize(x, p=2.0, dim=dim, eps=epsilon)\n",
    "\n",
    "d_dim = 128\n",
    "n_keys = 10\n",
    "pool_size = 100\n",
    "k_pool = keys_pool(n_keys,d_dim,pool_size)\n",
    "\n",
    "opt = torch.optim.Adam(k_pool.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3622171878814697\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = torch.randint(low=0,high=n_keys,size=(batch_size,))\n",
    "    opt.zero_grad()\n",
    "    out = k_pool(idx)\n",
    "    loss = loss_fn(out,idx)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 8, 7, 8, 5, 4, 4, 1, 0])\n",
      "tensor([1, 0, 8, 7, 8, 5, 4, 4, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "idx = torch.randint(low=0,high=n_keys,size=(10,))\n",
    "out = k_pool(idx)\n",
    "print(idx)\n",
    "print(out.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class keys_pool(nn.Module):\n",
    "    def __init__(self, input_size,d_dim,pool_size):\n",
    "        super().__init__()\n",
    "        self.key_encoder = nn.Linear(input_size,d_dim)\n",
    "        self.query       = nn.Parameter(torch.randn((pool_size,d_dim)))\n",
    "\n",
    "    def forward(self,x):\n",
    "        encoded_keys = self.key_encoder(x)\n",
    "      \n",
    "        prompt_norm  = self.l2_normalize(self.query  , dim=1)  # Pool_size, C\n",
    "        x_embed_norm = self.l2_normalize(encoded_keys, dim=1)  # B, C\n",
    "        similarity = torch.matmul(x_embed_norm, prompt_norm.t())  # B, Pool_size\n",
    "        idx = torch.argmax(similarity, dim=1)\n",
    "        return idx\n",
    "    @staticmethod\n",
    "    def l2_normalize(x, dim=None, epsilon=1e-12):\n",
    "        return torch.nn.functional.normalize(x, p=2.0, dim=dim, eps=epsilon)\n",
    "pool = keys_pool(128,128,100)\n",
    "x = torch.randn((10,128))\n",
    "out = pool(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 52, 47,  0, 52, 88,  6, 10, 71, 95])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
